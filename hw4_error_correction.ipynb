{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4. Error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Original reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bwa index \"./data/error_correction/MG1655-K12.first400K.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fastqc -o ./data/error_correction/fastqc/ \"./data/error_correction/ecoli_400K_err_1.fastq.gz\" \"./data/error_correction/ecoli_400K_err_2.fastq.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/per_base_quality_err.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bwa mem \"./data/error_correction/MG1655-K12.first400K.fasta\" \"./data/error_correction/ecoli_400K_err_1.fastq.gz\" \"./data/error_correction/ecoli_400K_err_2.fastq.gz\" 2>\"err.log\" | samtools view -bS -o \"./data/error_correction/ecoli_400K_err_aligned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools index \"./data/error_correction/ecoli_400K_err_aligned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gi|49175990|ref|NC_000913.2|\n"
     ]
    }
   ],
   "source": [
    "err_alignment = \"./data/error_correction/ecoli_400K_err_aligned.bam\"\n",
    "reference_path = \"./data/error_correction/MG1655-K12.first400K.fasta\"\n",
    "\n",
    "err_bam = pysam.AlignmentFile(err_alignment, \"rb\")\n",
    "contig = err_bam.references[0]\n",
    "ref_fasta = pysam.FastaFile(reference_path)\n",
    "seq = ref_fasta.fetch(contig)\n",
    "print(contig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм подстчета ошибок\n",
    "1) Берем пару изначальный рид, исправленный рид.  \n",
    "2) Если они не смаппились на один и тот же участок - пропускаем их.  \n",
    "3) Если смаппились, смотрим по одинаковым позициям на референсе:\n",
    "* Если основание обрезали, и оно не была верным матчем - Detected and removed error (true positive)  \n",
    "* Если основание обрезали, и оно была верным матчем - Incorrectly removed base (false positive)\n",
    "* Если основание не изменилось и было верным матчем - Correctly unmodified base (true negative)  \n",
    "* Если основание не изменилось и не было верным матчем - Undetected error (false negative)  \n",
    "* Если основание изменилось и было верным матчем - Falsely corrected error (false positive)  \n",
    "* Если основание изменилось и стало верным матчем, хотя раньше не было - Detected & corrected error (true positive)   \n",
    "* Отдельно в этих ридах смотрим на количество инсерций:   \n",
    "  если стало больше увеличиваем Undetected error (false negative) и Falsely corrected error (false positive)   \n",
    "  если меньше - Undetected error (false negative) и Detected and removed error (true positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_correction_stats(corrected_bam, error_bam, contig, seq):\n",
    "    indexed_corr_reads = pysam.IndexedReads(corrected_bam)\n",
    "    indexed_corr_reads.build()\n",
    "\n",
    "    d = {\n",
    "        'corrected_error': 0, 'removed_error': 0, \n",
    "        'undetected_error': 0, 'false_correction': 0, \n",
    "        'correctly_unmodified':0, 'incorrectly_removed': 0\n",
    "    }\n",
    "    unmapped = 0\n",
    "    diff_mapping = 0\n",
    "    total = 0\n",
    "    unpaired = 0\n",
    "    for read in err_bam.fetch(contig):\n",
    "        total += 1\n",
    "        err_matches = read.get_aligned_pairs(matches_only=False)\n",
    "        err_seq = read.query_sequence\n",
    "        try:\n",
    "            find_reads = indexed_corr_reads.find(read.query_name)\n",
    "        except KeyError:\n",
    "            # read was dropped during correction\n",
    "            unpaired += 1\n",
    "            continue\n",
    "            \n",
    "        cor_read = next(filter(lambda r: r.is_read1 == read.is_read1, find_reads))\n",
    "        if read.is_unmapped or cor_read.is_unmapped:\n",
    "            unmapped += 1\n",
    "            continue\n",
    "        if read.reference_end < cor_read.reference_end or cor_read.reference_start < read.reference_start:\n",
    "            diff_mapping += 1\n",
    "            continue\n",
    "\n",
    "        cor_seq = cor_read.query_sequence\n",
    "        if err_seq == cor_seq:\n",
    "            for read_pos, ref_pos in err_matches:\n",
    "                if read_pos is None or ref_pos is None or err_seq[read_pos] != seq[ref_pos]:\n",
    "                    d['undetected_error'] += 1\n",
    "                else:\n",
    "                    d['correctly_unmodified'] += 1\n",
    "            continue\n",
    "\n",
    "        cor_matches = cor_read.get_aligned_pairs(matches_only=False)\n",
    "\n",
    "        # count insertions\n",
    "        insertions_err = sum(int(ref_pos is None) for err_pos, ref_pos in err_matches)\n",
    "        insertions_cor = sum(int(ref_pos is None) for cor_pos, ref_pos in cor_matches)\n",
    "        if insertions_err < insertions_cor:\n",
    "            d['undetected_error'] += insertions_err\n",
    "            d['false_correction'] += insertions_cor - insertions_err\n",
    "        else:\n",
    "            d['undetected_error'] += insertions_cor\n",
    "            d['removed_error'] += insertions_err - insertions_cor\n",
    "\n",
    "        ref2err = {ref_pos: err_pos for err_pos, ref_pos in err_matches if ref_pos is not None}\n",
    "        ref2cor = {ref_pos: cor_pos for cor_pos, ref_pos in cor_matches if ref_pos is not None}\n",
    "        for pos in ref2err:\n",
    "            c_ref = seq[pos]\n",
    "            err_pos = ref2err[pos]\n",
    "            cor_pos = ref2cor.get(pos, '$')  # '$' marks that position was trimmed\n",
    "            \n",
    "            # if position trimmed\n",
    "            if cor_pos == '$' and err_pos is None:\n",
    "                d['removed_error'] += 1 \n",
    "            elif cor_pos == '$' and err_seq[err_pos] != c_ref:\n",
    "                d['removed_error'] += 1\n",
    "            elif cor_pos == '$' and err_seq[err_pos] == c_ref:\n",
    "                d['incorrectly_removed'] += 1\n",
    "\n",
    "            # if deletion\n",
    "            elif cor_pos is None and err_pos is None:\n",
    "                d['undetected_error'] += 1\n",
    "            elif cor_pos is None and err_seq[err_pos] != c_ref:\n",
    "                d['undetected_error'] += 1\n",
    "            elif cor_pos is None and err_seq[err_pos] == c_ref:\n",
    "                d['false_correction'] += 1\n",
    "\n",
    "            # if incorrect match\n",
    "            elif cor_seq[cor_pos] != c_ref and err_pos is None:\n",
    "                d['undetected_error'] += 1\n",
    "            elif cor_seq[cor_pos] != c_ref and err_seq[err_pos] != c_ref:\n",
    "                d['undetected_error'] += 1\n",
    "            elif cor_seq[cor_pos] != c_ref and err_seq[err_pos] == c_ref:\n",
    "                d['false_correction'] += 1\n",
    "\n",
    "            # if correct match\n",
    "            elif cor_seq[cor_pos] == c_ref and err_pos is None:\n",
    "                d['corrected_error'] += 1\n",
    "            elif cor_seq[cor_pos] == c_ref and err_seq[err_pos] != c_ref:\n",
    "                d['corrected_error'] += 1\n",
    "            elif cor_seq[cor_pos] == c_ref and err_seq[err_pos] == c_ref:\n",
    "                d['correctly_unmodified'] += 1\n",
    "            else:\n",
    "                print('I did not know there could be more cases ;(')\n",
    "            \n",
    "    read_stats = {'n_reads': total, 'unmapped': unmapped, 'position_changed': diff_mapping, 'pair_dropped': unpaired}\n",
    "    return d, read_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Trimmomatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Trimmomatic PE -phred33 ./data/error_correction/ecoli_400K_err_1.fastq.gz ./data/error_correction/ecoli_400K_err_2.fastq.gz -baseout ./data/error_correction/ecoli_400K_trimmed LEADING:10 TRAILING:10 SLIDINGWINDOW:10:10 MINLEN:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fastqc -o ./data/error_correction/fastqc/ \"./data/error_correction/ecoli_400K_trimmed_1P\" \"./data/error_correction/ecoli_400K_trimmed_2P\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/per_base_quality_trim.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bwa mem \"./data/error_correction/MG1655-K12.first400K.fasta\" \"./data/error_correction/ecoli_400K_trimmed_1P\" \"./data/error_correction/ecoli_400K_trimmed_2P\" 2>\"trim.log\" | samtools view -bS - | samtools sort -o \"./data/error_correction/ecoli_400K_trim_aligned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools index \"./data/error_correction/ecoli_400K_trim_aligned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_alignment = \"./data/error_correction/ecoli_400K_trim_aligned.bam\"\n",
    "trim_bam = pysam.AlignmentFile(trim_alignment, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(error_bam, corrected_bam, reference_fasta, contig):\n",
    "    cor_pileup = corrected_bam.pileup(contig, fastafile=reference_fasta)\n",
    "    err_pileup = error_bam.pileup(contig, fastafile=reference_fasta)\n",
    "\n",
    "    cor_count = Counter()\n",
    "    err_count = Counter()\n",
    "\n",
    "    for cor_col, err_col in zip(cor_pileup, err_pileup):\n",
    "        cor_match = cor_col.get_query_sequences(mark_matches=True, add_indels=True)\n",
    "        err_match = err_col.get_query_sequences(mark_matches=True, add_indels=True)\n",
    "    \n",
    "        cor_count.update(cor_match)\n",
    "        err_count.update(err_match)\n",
    "    \n",
    "#     print('Corrected reads: ', cor_count)\n",
    "    print('Orignal reads: ', sum(err_count.values()))\n",
    "    \n",
    "    d = {}\n",
    "    y = err_count - cor_count\n",
    "    x = cor_count - err_count\n",
    "    total_removed = sum(err_count.values()) - sum(cor_count.values())\n",
    "    d['corrected_error'] = x[','] + x['.']\n",
    "    d['removed_error'] = sum(y[key] for key in y if key not in ['.', ','])\n",
    "    d['undetected_error'] = sum(min(cor_count[key], err_count[key]) for key in err_count if key not in ['.', ','])\n",
    "    d['false_correction'] = sum(x[key] for key in x if key not in [',', '.'])\n",
    "    d['correctly_unmodified'] = min(cor_count['.'], err_count['.']) + min(cor_count[','], err_count[','])\n",
    "    d['incorrectly_removed'] = total_removed - d['removed_error']\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_stats(err_bam, trim_bam, ref_fasta, contig)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrected_error': 508, 'removed_error': 6538995, 'undetected_error': 1226955, 'false_correction': 22237, 'correctly_unmodified': 246256706, 'incorrectly_removed': 14665349}\n",
      "{'n_reads': 2763102, 'unmapped': 882, 'position_changed': 38622, 'pair_dropped': 36736}\n"
     ]
    }
   ],
   "source": [
    "d, read_stats = error_correction_stats(trim_bam, err_bam, contig, seq)\n",
    "print(d)\n",
    "print(read_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Error in corrected reads | Correct base in corrected reads| Base is absent in corrected reads |\n",
    "|--|--|--|--|\n",
    "|Error in raw data       | 1226955 | 508       | 6538995  |\n",
    "|Correct base in raw data| 22237   | 246256706 | 14665349 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error % after correction: 0.50 %\n",
      "Error % before correction: 2.89 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Error % after correction: {100 * (1226955 + 22237) /  (1226955 + 22237 + 246256706 + 508):.2f} %') \n",
    "print(f'Error % before correction: {100 * (1226955 + 508 + 6538995) / (1226955 + 508 + 6538995 + 22237 + 246256706 + 14665349):.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Trimmomatic + BayesHammer (SPAdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line: /Users/b2w/Prog/tools/SPAdes-3.13.0-Darwin/bin/spades.py\t-1\t/Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_1P.fq\t-2\t/Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_2P.fq\t--only-error-correction\t-o\t/Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades\t\n",
      "\n",
      "System information:\n",
      "  SPAdes version: 3.13.0\n",
      "  Python version: 3.7.4\n",
      "  OS: Darwin-19.3.0-x86_64-i386-64bit\n",
      "\n",
      "Output dir: /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades\n",
      "Mode: ONLY read error correction (without assembling)\n",
      "Debug mode is turned OFF\n",
      "\n",
      "Dataset parameters:\n",
      "  Multi-cell mode (you should set '--sc' flag if input data was obtained with MDA (single-cell) technology or --meta flag if processing metagenomic dataset)\n",
      "  Reads:\n",
      "    Library number: 1, library type: paired-end\n",
      "      orientation: fr\n",
      "      left reads: ['/Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_1P.fq']\n",
      "      right reads: ['/Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_2P.fq']\n",
      "      interlaced reads: not specified\n",
      "      single reads: not specified\n",
      "      merged reads: not specified\n",
      "Read error correction parameters:\n",
      "  Iterations: 1\n",
      "  PHRED offset will be auto-detected\n",
      "  Corrected reads will be compressed\n",
      "Other parameters:\n",
      "  Dir for temp files: /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/tmp\n",
      "  Threads: 16\n",
      "  Memory limit (in Gb): 250\n",
      "\n",
      "\n",
      "======= SPAdes pipeline started. Log can be found here: /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/spades.log\n",
      "\n",
      "\n",
      "===== Read error correction started. \n",
      "\n",
      "\n",
      "== Running read error correction tool: /Users/b2w/Prog/tools/SPAdes-3.13.0-Darwin/bin/spades-hammer /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/corrected/configs/config.info\n",
      "\n",
      "  0:00:00.000     4M / 4M    INFO    General                 (main.cpp                  :  75)   Starting BayesHammer, built from N/A, git revision N/A\n",
      "  0:00:00.006     4M / 4M    INFO    General                 (main.cpp                  :  76)   Loading config from /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/corrected/configs/config.info\n",
      "  0:00:00.014     4M / 4M    INFO    General                 (main.cpp                  :  78)   Maximum # of threads to use (adjusted due to OMP capabilities): 4\n",
      "  0:00:00.014     4M / 4M    INFO    General                 (memory_limit.cpp          :  49)   Memory limit set to 250 Gb\n",
      "  0:00:00.014     4M / 4M    INFO    General                 (main.cpp                  :  86)   Trying to determine PHRED offset\n",
      "  0:00:00.015     4M / 4M    INFO    General                 (main.cpp                  :  92)   Determined value is 33\n",
      "  0:00:00.015     4M / 4M    INFO    General                 (hammer_tools.cpp          :  36)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]\n",
      "  0:00:00.015     4M / 4M    INFO    General                 (main.cpp                  : 113)   Size of aux. kmer data 24 bytes\n",
      "     === ITERATION 0 begins ===\n",
      "  0:00:00.022     4M / 4M    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 301)   Building kmer index\n",
      "  0:00:00.022     4M / 4M    INFO    General                 (kmer_index_builder.hpp    : 117)   Splitting kmer instances into 64 files using 4 threads. This might take a while.\n",
      "  0:00:00.022     4M / 4M    INFO    General                 (file_limit.hpp            :  32)   Open file limit set to 256\n",
      "  0:00:00.022     4M / 4M    INFO    General                 (kmer_splitters.hpp        :  89)   Memory available for splitting buffers: 20.833 Gb\n",
      "  0:00:00.022     4M / 4M    INFO    General                 (kmer_splitters.hpp        :  97)   Using cell size of 1048576\n",
      "  0:00:00.023     3G / 3G    INFO   K-mer Splitting          (kmer_data.cpp             :  97)   Processing /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_1P.fq\n",
      "  0:00:16.496     3G / 3G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 1363202 reads\n",
      "  0:00:16.497     3G / 3G    INFO   K-mer Splitting          (kmer_data.cpp             :  97)   Processing /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_2P.fq\n",
      "  0:00:33.756     3G / 3G    INFO   K-mer Splitting          (kmer_data.cpp             : 107)   Processed 2726404 reads\n",
      "  0:00:33.756     3G / 3G    INFO   K-mer Splitting          (kmer_data.cpp             : 112)   Total 2726404 reads processed\n",
      "  0:00:33.867    16M / 3G    INFO    General                 (kmer_index_builder.hpp    : 120)   Starting k-mer counting.\n",
      "  0:00:35.084    16M / 3G    INFO    General                 (kmer_index_builder.hpp    : 127)   K-mer counting done. There are 13319700 kmers in total.\n",
      "  0:00:35.084    16M / 3G    INFO    General                 (kmer_index_builder.hpp    : 133)   Merging temporary buckets.\n",
      "  0:00:35.377    16M / 3G    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 314)   Building perfect hash indices\n",
      "  0:00:36.301    16M / 3G    INFO    General                 (kmer_index_builder.hpp    : 150)   Merging final buckets.\n",
      "  0:00:36.543    16M / 3G    INFO   K-mer Index Building     (kmer_index_builder.hpp    : 336)   Index built. Total 6184232 bytes occupied (3.71434 bits per kmer).\n",
      "  0:00:36.544    16M / 3G    INFO   K-mer Counting           (kmer_data.cpp             : 356)   Arranging kmers in hash map order\n",
      "  0:00:37.179   224M / 3G    INFO    General                 (main.cpp                  : 148)   Clustering Hamming graph.\n",
      "  0:01:19.770   224M / 3G    INFO    General                 (main.cpp                  : 155)   Extracting clusters\n",
      "  0:01:26.617   224M / 3G    INFO    General                 (main.cpp                  : 167)   Clustering done. Total clusters: 2162676\n",
      "  0:01:26.621   120M / 3G    INFO   K-mer Counting           (kmer_data.cpp             : 376)   Collecting K-mer information, this takes a while.\n",
      "  0:01:26.752   428M / 3G    INFO   K-mer Counting           (kmer_data.cpp             : 382)   Processing /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_1P.fq\n",
      "  0:02:08.707   428M / 3G    INFO   K-mer Counting           (kmer_data.cpp             : 382)   Processing /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_2P.fq\n",
      "  0:02:52.004   428M / 3G    INFO   K-mer Counting           (kmer_data.cpp             : 389)   Collection done, postprocessing.\n",
      "  0:02:52.049   428M / 3G    INFO   K-mer Counting           (kmer_data.cpp             : 403)   There are 13319700 kmers in total. Among them 9705008 (72.8621%) are singletons.\n",
      "  0:02:52.049   428M / 3G    INFO    General                 (main.cpp                  : 173)   Subclustering Hamming graph\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 649)   Subclustering done. Total 1 non-read kmers were generated.\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 650)   Subclustering statistics:\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 651)     Total singleton hamming clusters: 1368742. Among them 2630 (0.192147%) are good\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 652)     Total singleton subclusters: 4154. Among them 4152 (99.9519%) are good\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 653)     Total non-singleton subcluster centers: 855069. Among them 836230 (97.7968%) are good\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 654)     Average size of non-trivial subcluster: 13.9766 kmers\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 655)     Average number of sub-clusters per non-singleton cluster: 1.08223\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 656)     Total solid k-mers: 843012\n",
      "  0:02:58.186   428M / 3G    INFO   Hamming Subclustering    (kmer_cluster.cpp          : 657)     Substitution probabilities: [4,4]((0.925355,0.0418834,0.024675,0.00808684),(0.0168157,0.960472,0.0122082,0.0105037),(0.0104769,0.0121846,0.960491,0.016848),(0.00806882,0.0246493,0.0418457,0.925436))\n",
      "  0:02:58.188   428M / 3G    INFO    General                 (main.cpp                  : 178)   Finished clustering.\n",
      "  0:02:58.188   428M / 3G    INFO    General                 (main.cpp                  : 197)   Starting solid k-mers expansion in 4 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0:03:29.448   428M / 3G    INFO    General                 (main.cpp                  : 218)   Solid k-mers iteration 0 produced 44355 new k-mers.\n",
      "  0:03:55.566   428M / 3G    INFO    General                 (main.cpp                  : 218)   Solid k-mers iteration 1 produced 1633 new k-mers.\n",
      "  0:04:22.285   428M / 3G    INFO    General                 (main.cpp                  : 218)   Solid k-mers iteration 2 produced 11 new k-mers.\n",
      "  0:04:46.746   428M / 3G    INFO    General                 (main.cpp                  : 218)   Solid k-mers iteration 3 produced 0 new k-mers.\n",
      "  0:04:46.746   428M / 3G    INFO    General                 (main.cpp                  : 222)   Solid k-mers finalized\n",
      "  0:04:46.747   428M / 3G    INFO    General                 (hammer_tools.cpp          : 220)   Starting read correction in 4 threads.\n",
      "  0:04:46.747   428M / 3G    INFO    General                 (hammer_tools.cpp          : 233)   Correcting pair of reads: /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_1P.fq and /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/ecoli_400K_trimmed_2P.fq\n",
      "  0:04:47.910   700M / 3G    INFO    General                 (hammer_tools.cpp          : 168)   Prepared batch 0 of 400000 reads.\n",
      "  0:04:54.456   712M / 3G    INFO    General                 (hammer_tools.cpp          : 175)   Processed batch 0\n",
      "  0:04:55.646   712M / 3G    INFO    General                 (hammer_tools.cpp          : 185)   Written batch 0\n",
      "  0:04:56.753   796M / 3G    INFO    General                 (hammer_tools.cpp          : 168)   Prepared batch 1 of 400000 reads.\n",
      "  0:05:03.390   796M / 3G    INFO    General                 (hammer_tools.cpp          : 175)   Processed batch 1\n",
      "  0:05:04.660   796M / 3G    INFO    General                 (hammer_tools.cpp          : 185)   Written batch 1\n",
      "  0:05:05.709   816M / 3G    INFO    General                 (hammer_tools.cpp          : 168)   Prepared batch 2 of 400000 reads.\n",
      "  0:05:13.332   832M / 3G    INFO    General                 (hammer_tools.cpp          : 175)   Processed batch 2\n",
      "  0:05:14.736   832M / 3G    INFO    General                 (hammer_tools.cpp          : 185)   Written batch 2\n",
      "  0:05:15.183   828M / 3G    INFO    General                 (hammer_tools.cpp          : 168)   Prepared batch 3 of 163202 reads.\n",
      "  0:05:18.764   828M / 3G    INFO    General                 (hammer_tools.cpp          : 175)   Processed batch 3\n",
      "  0:05:19.540   828M / 3G    INFO    General                 (hammer_tools.cpp          : 185)   Written batch 3\n",
      "  0:05:19.846   428M / 3G    INFO    General                 (hammer_tools.cpp          : 274)   Correction done. Changed 871780 bases in 644398 reads.\n",
      "  0:05:19.846   428M / 3G    INFO    General                 (hammer_tools.cpp          : 275)   Failed to correct 16363 bases out of 250804813.\n",
      "  0:05:19.869    16M / 3G    INFO    General                 (main.cpp                  : 255)   Saving corrected dataset description to /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/corrected/corrected.yaml\n",
      "  0:05:19.872    16M / 3G    INFO    General                 (main.cpp                  : 262)   All done. Exiting.\n",
      "\n",
      "== Compressing corrected reads (with pigz)\n",
      "\n",
      "== Dataset description file was created: /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/corrected/corrected.yaml\n",
      "\n",
      "\n",
      "===== Read error correction finished. \n",
      "\n",
      " * Corrected reads are in /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/corrected/\n",
      "\n",
      "======= SPAdes pipeline finished.\n",
      "\n",
      "SPAdes log can be found here: /Users/b2w/PycharmProjects/BioinfInstitute/NGS2020/data/error_correction/spades/spades.log\n",
      "\n",
      "Thank you for using SPAdes!\n"
     ]
    }
   ],
   "source": [
    "!spades.py -1 ./data/error_correction/ecoli_400K_trimmed_1P.fq -2 ./data/error_correction/ecoli_400K_trimmed_2P.fq --only-error-correction -o ./data/error_correction/spades 2> spades_bh.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started analysis of ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 5% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 10% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 15% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 20% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 25% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 30% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 35% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 40% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 45% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 50% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 55% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 60% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 65% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 70% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 75% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 80% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 85% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 90% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Approx 95% complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Analysis complete for ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\n",
      "Started analysis of ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 5% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 10% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 15% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 20% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 25% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 30% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 35% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 40% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 45% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 50% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 55% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 60% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 65% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 70% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 75% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 80% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 85% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 90% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Approx 95% complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n",
      "Analysis complete for ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "!fastqc -o ./data/error_correction/fastqc/ \"./data/error_correction/spades/corrected/ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\" \"./data/error_correction/spades/corrected/ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bwa mem \"./data/error_correction/MG1655-K12.first400K.fasta\" \"./data/error_correction/spades/corrected/ecoli_400K_trimmed_1P.00.0_0.cor.fastq.gz\" \"./data/error_correction/spades/corrected/ecoli_400K_trimmed_2P.00.0_0.cor.fastq.gz\" 2>\"bh.log\" | samtools view -bS - | samtools sort -o \"./data/error_correction/ecoli_400K_bayes_aligned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools index \"./data/error_correction/ecoli_400K_bayes_aligned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_alignment = \"./data/error_correction/ecoli_400K_bayes_aligned.bam\"\n",
    "bh_bam = pysam.AlignmentFile(bh_alignment, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrected_error': 816402, 'removed_error': 6307000, 'undetected_error': 320252, 'false_correction': 10348, 'correctly_unmodified': 245089820, 'incorrectly_removed': 14399767}\n",
      "{'n_reads': 2763102, 'unmapped': 90, 'position_changed': 46036, 'pair_dropped': 47662}\n"
     ]
    }
   ],
   "source": [
    "bh_correction_stats, bh_read_stats = error_correction_stats(bh_bam, err_bam, contig, seq)\n",
    "print(bh_correction_stats)\n",
    "print(bh_read_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Error in corrected reads | Correct base in corrected reads| Base is absent in corrected reads |\n",
    "|--|--|--|--|\n",
    "|Error in raw data       | 320252   | 816402      | 6307000  |\n",
    "|Correct base in raw data| 10348  | 245089820 | 14399767 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error % after correction: 0.13 %\n",
      "Error % before correction: 2.79 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Error % after correction: {100 * (320252 + 10348) /  (320252 + 816402 + 10348 + 245089820):.2f} %') \n",
    "print(f'Error % before correction: {100 * (320252 + 816402 + 6307000) / (320252 + 816402 + 6307000 + 10348 + 245089820 + 14399767):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
